MUST TEST
1. add HIGH, LOW, CLOSE, VOL, Скользящие средние
2. Candle Number
3. log(C(t)/C(t+1)) use Instead of delta... логарифм относительного приращения
   или просто C(t)/C(t+1)
4. Wavwlet decomposition ??? или ну его , сильно сложно и возможно неоправданно
5. Использовать ансамбль (например на разных интервалах, или архитектурах)
   10, 15, 30, 1H, 1D, 1W
   


ADX,ATR добавить в state!!?
http://bottlepy.org/docs/dev/tutorial.html#introducing-formsdict
https://habr.com/ru/post/250831/
http://python.su/forum/topic/7215/?page=1#post-48902
https://automated-testing.info/t/zametka-prostoj-i-udobnyj-web-framework-bottle/3973
https://habr.com/ru/post/221659/

http://o-s-a.net/os-engine.html
lisicin
lde@mail.ru
Su....3
https://jamesmccaffrey.wordpress.com/2018/03/06/using-a-neural-network-created-by-cntk-or-tensorflow-in-a-c-program/
разложить на шум сезонность и тренд 1 5 10 15 30 60 1D

Проверить стационарность

https://yandex.ru/search/?lr=213&text=Reinforcement%20Learning%3A%20With%20Open%20AI%2C%20TensorFlow%20and%20Keras%20Using%20Python%20pdf
https://github.com/topics/stock-price-prediction?o=desc&s=stars
https://towardsdatascience.com/neural-networks-to-predict-the-market-c4861b649371
построить пример https://www.quantinsti.com/blog/artificial-neural-network-python-using-keras-predicting-stock-price-movement?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com
посмотреть https://gitlab.com/doctorj/sairen/
Посмотреть обязательно reinforcement learning: I have been eying this paper: "A Deep Reinforcement Learning Framework for the Financial Portfolio" that you may enjoy, my hope is to build my own December 2018. Here is the authors accompanying Github repo for the project as well: https://github.com/ZhengyaoJiang/PGPortfolio
очень интересные комменты прочитать http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/  и это прочитать из комментов https://arxiv.org/pdf/1710.02298.pdf

293045971 

https://github.com/germain-hug/Deep-RL-Keras
#https://github.com/germain-hug/Deep-RL-Keras
""" Deep Q-Learning for OpenAI Gym environment
"""

import os
import sys
import gym
import argparse
import numpy as np
import pandas as pd
import tensorflow as tf

#from A2C.a2c import A2C
#from A3C.a3c import A3C
from DDQN.ddqn import DDQN
#from DDPG.ddpg import DDPG

from keras.backend.tensorflow_backend import set_session
from keras.utils import to_categorical

#from utils.atari_environment import AtariEnvironment
#from utils.continuous_environments import Environment
#from utils.networks import get_session

#gym.logger.set_level(40)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

def parse_args(args):
    """ Parse arguments from command line input
    """
    parser = argparse.ArgumentParser(description='Training parameters')
    #
    parser.add_argument('--type', type=str, default='DDQN',help="Algorithm to train from {A2C, A3C, DDQN, DDPG}")
    #parser.add_argument('--is_atari', dest='is_atari', action='store_true', help="Atari Environment")
    parser.add_argument('--with_PER', dest='with_per', action='store_true', help="Use Prioritized Experience Replay (DDQN + PER)")
    parser.add_argument('--dueling', dest='dueling', action='store_true', help="Use a Dueling Architecture (DDQN)")
    #
    parser.add_argument('--nb_episodes', type=int, default=50, help="Number of training episodes")
    parser.add_argument('--batch_size', type=int, default=20, help="Batch size (experience replay)")
    #parser.add_argument('--consecutive_frames', type=int, default=4, help="Number of consecutive frames (action repeat)")
    #parser.add_argument('--training_interval', type=int, default=30, help="Network training frequency")
    #parser.add_argument('--n_threads', type=int, default=8, help="Number of threads (A3C)")
    #
    parser.add_argument('--gather_stats', dest='gather_stats', action='store_true',help="Compute Average reward per episode (slower)")
    #parser.add_argument('--render', dest='render', action='store_true', help="Render environment while training")
    #parser.add_argument('--env', type=str, default='BreakoutNoFrameskip-v4',help="OpenAI Gym Environment")
    parser.add_argument('--gpu', type=int, default=0, help='GPU ID')
    #
    parser.set_defaults(render=False)
    return parser.parse_args(args)

def main(args=None):

    # Parse arguments
    if args is None:
        args = sys.argv[1:]
    args = parse_args(args)

    # Check if a GPU ID was set
    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    set_session(get_session())
    summary_writer = tf.summary.FileWriter(args.type + "/tensorboard_" + args.type)

    # Environment Initialization
    '''if(args.is_atari):
        # Atari Environment Wrapper
        env = AtariEnvironment(args)
        state_dim = env.get_state_size()
        action_dim = env.get_action_size()
    elif(args.type=="DDPG"):
        # Continuous Environments Wrapper
        env = Environment(gym.make(args.env), args.consecutive_frames)
        env.reset()
        state_dim = env.get_state_size()
        action_space = gym.make(args.env).action_space
        action_dim = action_space.high.shape[0]
        act_range = action_space.high
    else:'''
        
	# Standard Environments
    env = Environment(gym.make(args.env), args.consecutive_frames)
    env.reset()
    state_dim = env.get_state_size()
    action_dim = gym.make(args.env).action_space.n

    # Pick algorithm to train
    if(args.type=="DDQN"):
        algo = DDQN(action_dim, state_dim, args)
    #elif(args.type=="A2C"):
    #    algo = A2C(action_dim, state_dim, args.consecutive_frames)
    #elif(args.type=="A3C"):
    #    algo = A3C(action_dim, state_dim, args.consecutive_frames, is_atari=args.is_atari)
    #elif(args.type=="DDPG"):
    #    algo = DDPG(action_dim, state_dim, act_range, args.consecutive_frames)

    # Train
    stats = algo.train(env, args, summary_writer)

    # Export results to CSV
    if(args.gather_stats):
        df = pd.DataFrame(np.array(stats))
        df.to_csv(args.type + "/logs.csv", header=['Episode', 'Mean', 'Stddev'], float_format='%10.5f')

    # Display agent
    old_state, time = env.reset(), 0
    while True:
        env.render()
        a = algo.policy_action(old_state)
        old_state, r, done, _ = env.step(a)
        time += 1
        if done: env.reset()


if __name__ == "__main__":
    main()
18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 
18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 
18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 
18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 18.01.2019 
DDQN.py
 #158 Save Model
	agent.saveModel("../models/model_ep","")
		
		

Environment.py
допилить
def GetStockDataVecFN(self,key =  r'D:\PycharmProjects\RIZ8\SPFB.RTS-6.18(5M).csv',append=False):
            dateparse = lambda x, y: pd.datetime.combine(pd.datetime.strptime(x, '%Y%m%d'),
                                                         pd.datetime.strptime(y, '%H%M%S').time())
            df = pd.read_csv(key, delimiter=',', index_col=['datetime'],
                               parse_dates={'datetime': ['<DATE>', '<TIME>']}, date_parser=dateparse)
            if append:  
				self.data = pd.concat(self.data, df, ignore_index=True)
			Else 
				self.data = df
				
def reset(self):
    self.t = 0
    self.done = False
    self.profits = 0
    self.position = 0
    # self.position_value = 0
    self.history = [0 for _ in range(self.history_t)]
	
	self.step(0) for _ in range(self.history_t) #step(0) - act = 0: stay
    return np.array([0] + self.history), 0, self.done,




def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
	
DDQN.py
Добавить Train
		
		
score = tfSummary('score'+window_size+volume+{если evaluate то E=+1}, cumul_reward)
            summary_writer.add_summary(score, global_step=e)

def Evaluate(self, env, args, summary_writer, model = "",andtrain=True):
        """ Main DDQN Training Algorithm
        """

        results = []
        #tqdm_e = tqdm(range(args.nb_episodes), desc='Score', leave=True, unit=" episodes")
        step=0
        #gross_profit = 0
        #for e in tqdm_e:
            # Reset episode
            time, cumul_reward, done  = 0, 0, False
            old_state = env.reset()
            ##########################################
            total_reward = 0
            total_profit = 0
            total_loss = 0
            total_profitMax = 0
            total_profitMin = 0
            max_drop = 0
            profitLst = []
            lossLst = []
            step = 0
            #####################################3####
            while not done:
                #if args.render: env.render()
                # Actor picks an action (following the policy)
                a = self.policy_action(old_state)
                # Retrieve new state, reward, and whether the state is terminal
                #new_state, r, done, _ = env.step(a)

                #######################################################
                new_state, r, done, buy, sell, profit = env.step(a)

                total_reward += r
                if profit != 0:
                    total_profit += profit
                    if total_profit > total_profitMax:
                        total_profitMax = total_profit
                        total_profitMin = total_profit
                    if total_profit < total_profitMin:
                        total_profitMin = total_profit
                        try:
                            if max_drop < (total_profitMax - total_profitMin) / total_profitMax:
                                max_drop = (total_profitMax - total_profitMin) / total_profitMax
                        except:
                            max_drop=0


                if profit > 0:
                    profitLst.append(profit)
                elif profit < 0:
                    lossLst.append(profit)

                step += 1
                if step % 1500 == 0:
                    print('maxProfit: {} maxLOSS: {} avgProfit: {:01.2f} avgLOSS: {:01.2f} maxdrop: {:.2%} Total profit: {}/{}  '.format(
                        np.max(profitLst), -np.min(lossLst), np.mean(profitLst), -np.mean(lossLst),
                        max_drop, total_profit, gross_profit))
                done = True if step == len(env.data) - 2 else False
                ######################################################
                # Memorize for experience replay
				if andTrain
					self.memorize(old_state, a, r, done, new_state)
					# Train DDQN and transfer weights to target network
					if(self.buffer.size() > args.batch_size):
					    self.train_agent(args.batch_size)
					    self.agent.transfer_weights()
				# Update current state
                old_state = new_state
                cumul_reward += r
                time += 1
				
            gross_profit += total_profit
            # Gather stats every episode for plotting
            if(args.gather_stats):
                mean, stdev = gather_stats(self, env)
                results.append([e, mean, stdev])

            # Export results for Tensorboard
            score = tfSummary('score', cumul_reward)
            summary_writer.add_summary(score, global_step=e)
            summary_writer.flush()
            вот тут summary writer поправить
			
            # Display score
            #tqdm_e.set_description("Score: " + str(cumul_reward))
            #tqdm_e.refresh()

        return results				
Agent.Py
	def saveModel(self, filename,version)
	!!!import os.path
        if version:
			self.model.save(file_path+version)
		else:
			v=1
			while os.path.exists(file_path+v):
				v += 1
			self.model.save(file_path+string(v))
	def loadModel(self, filename,version)
	if version:
			self.model.load(file_path+version)
		else:
		v=1
		while os.path.exists(file_path+v):
			v += 1
			
